# Research Extensions

This document outlines planned experimental variations on the ConQuer framework [Fu et al., 2025](https://arxiv.org/abs/2503.14662) using this implementation as research infrastructure.

## Motivation

The original ConQuer framework demonstrates the efficacy of concept-based retrieval for grounding quiz generation in factual knowledge sources. However, several research questions remain open regarding optimal implementation choices and potential extensions. This implementation provides a modular codebase to systematically investigate variations across four dimensions: question generation approaches, model selection, retrieval strategies, and evaluation methodologies.

## Question Generation Variations

We propose to investigate extensions beyond the multiple-choice format employed in the original framework. Whilst multiple-choice questions provide straightforward automated evaluation, alternative question types may better assess different aspects of student understanding.

Short-answer questions require students to generate responses rather than selecting from provided options, potentially reducing guessing and testing deeper comprehension. We will explore prompt engineering strategies for generating short-answer questions with verifiable model answers, alongside automated evaluation techniques comparing student responses to expected answers.

True/false statements offer a simpler format that may be more appropriate for certain education levels or factual knowledge assessment. The challenge lies in generating plausible false statements that test genuine understanding rather than superficial pattern matching.

Ordering and sequencing tasks (for example, "arrange these events chronologically" or "rank these concepts by specificity") assess relational understanding and may be particularly effective for historical or procedural domains. Implementation requires structured knowledge extraction from Wikipedia content.

Fill-in-the-blank exercises combine aspects of short-answer questions (requiring generation) with constrained responses (single word or phrase). These may be particularly suitable for technical vocabulary acquisition and can be automatically generated by masking key terms in Wikipedia summaries.

For each question type, we will evaluate generation quality, difficulty calibration across education levels, and correlation with student learning outcomes if human evaluation data becomes available.

## Model Comparison Framework

The original ConQuer implementation does not systematically compare different language models for quiz generation and evaluation. We propose comprehensive benchmarking across open-source and proprietary models to understand performance-cost trade-offs and identify domain-specific strengths.

Open-source models provide privacy, cost control, and reproducibility benefits. We will evaluate recent models of various sizes and capabilities, comparing dimensions such as quiz quality (using the existing five-dimension evaluation framework), generation speed, memory requirements, and consistency across runs.

Proprietary models typically demonstrate superior performance but incur costs and raise privacy concerns. We will benchmark these models against the open-source baseline to quantify the quality gap and assess whether the cost differential is justified for educational applications.

Model specialisation may vary by subject domain. We hypothesise that models with stronger coding knowledge may generate higher-quality quizzes for computer science, whilst models trained on diverse web content may excel at humanities topics. Cross-domain analysis will identify these patterns.

Fine-tuning experiments will explore whether domain-specific adaptation improves quiz quality. Using the ConQuer evaluation framework, we can create supervised datasets of high-quality quizzes (those scoring highly across all dimensions) for continued training. We will investigate both standard fine-tuning approaches and more parameter-efficient methods.

## Indexing and Retrieval Techniques

The current implementation uses semantic similarity search via embeddings to retrieve relevant Wikipedia content. We propose systematic comparison of retrieval strategies to identify optimal approaches for educational content generation.

Alternative retrieval methods based on keyword matching and term frequency provide strong lexical matching and interpretable relevance scoring. Whilst potentially less effective for semantic similarity, they may outperform embedding-based approaches for factual queries with specific terminology. We will implement and compare these approaches using LlamaIndex's retrieval components.

Hybrid retrieval approaches combine keyword matching with semantic similarity, potentially capturing both exact term matches and conceptual relevance. LlamaIndex supports hybrid retrieval by combining multiple retrievers. We will investigate different combination strategies to optimise retrieval quality.

Chunking strategies significantly impact retrieval effectiveness. The current implementation uses fixed-size character chunks with overlap. We will experiment with sentence-based chunking, paragraph-based chunking, and semantic chunking that splits content at topic boundaries. Additionally, we will investigate hierarchical approaches that maintain document structure and context.

Embedding model comparison will evaluate different models for concept-based retrieval. Metrics include retrieval precision (relevance of returned chunks to the question), coverage (whether key information is retrieved), and downstream quiz quality.

Re-ranking approaches apply a second evaluation stage to improve initial retrieval results. We will implement re-ranking methods to assess whether the additional computational cost yields sufficient quality improvements for educational applications.

Index persistence and optimisation using specialised vector storage systems may improve retrieval speed and enable larger knowledge bases. We will benchmark these systems against the current in-memory approach for scalability and latency characteristics.

## Evaluation Methodology

The original framework's evaluation provides efficient quality assessment but lacks validation against expert annotations or student learning outcomes. We propose enhanced evaluation approaches to strengthen the framework's empirical foundations.

Human expert validation will collect educator annotations on a sample of generated quizzes across subjects and education levels. This enables assessment of agreement between human judges and the automated evaluator, providing ground-truth data to calibrate the automated evaluation and identify systematic biases.

Student learning outcomes represent the ultimate validation criterion. Working with educational institutions, we aim to deploy quizzes in real classroom settings and measure pre-test and post-test performance, engagement metrics such as completion rates and time spent, and student feedback on perceived quality and difficulty.

Adversarial evaluation will probe failure modes by intentionally generating low-quality quizzes (incorrect answers, off-topic content, inappropriate difficulty) to verify the evaluation framework correctly identifies problems. This stress-testing reveals evaluation blind spots.

Cross-domain analysis will investigate whether quiz quality varies systematically by subject area, potentially indicating Wikipedia coverage gaps or domain-specific challenges in concept extraction and question generation.

Additional evaluation dimensions beyond the current five may capture important quality aspects. Candidates include clarity (question readability and lack of ambiguity), fairness (avoiding cultural bias or requiring unstated assumptions), and pedagogical alignment (whether questions target appropriate learning objectives for the topic and level).

## Knowledge Source Extensions

Whilst Wikipedia provides broad, reliable coverage, domain-specific sources may improve quiz quality for specialised subjects. We will investigate retrieval and combination strategies for multiple knowledge sources.

Textbook integration using open educational resources may provide more pedagogically structured content than Wikipedia articles, potentially improving alignment with curriculum standards.

Research literature from open-access repositories could support advanced quizzes (PhD level) in scientific domains where Wikipedia coverage is limited.

Multi-source integration raises challenges in ranking and synthesising information from heterogeneous sources with varying reliability and pedagogical appropriateness. We will explore strategies for combining information from multiple sources whilst maintaining quality and coherence.

## References

Fu, Y., Wang, Z., Yang, L., Huo, M., & Dai, Z. (2025). ConQuer: A Framework for Concept-Based Quiz Generation. *arXiv preprint arXiv:2503.14662*. https://arxiv.org/abs/2503.14662
